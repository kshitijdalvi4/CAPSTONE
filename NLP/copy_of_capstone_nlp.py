# -*- coding: utf-8 -*-
"""Copy of Capstone_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13dIcTOXkXqOO9Ub05OIBaHmkSl_h8Oyz

Book Upload Via Drive Preferred

# Single Chunk
"""

# DSA NLP Question Generation System for Google Colab
# Run each cell sequentially in Google Colab

# ==========================================
# CELL 1: Install Required Packages
# ==========================================

!pip install transformers torch sentence-transformers
!pip install chromadb
!pip install PyPDF2 python-docx
!pip install langchain langchain-community
!pip install gradio
!pip install nltk spacy
!python -m spacy download en_core_web_sm

# ==========================================
# CELL 2: Import Libraries and Setup
# ==========================================

import os
import json
import random
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple
import re
from pathlib import Path

# NLP and ML libraries
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from sentence_transformers import SentenceTransformer
import nltk
import spacy

# Vector database
import chromadb
from chromadb.config import Settings

# Document processing
import PyPDF2
from docx import Document

# Web interface
import gradio as gr

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

print("âœ… All packages installed and imported successfully!")

# ==========================================
# CELL 3: Initialize Models
# ==========================================

class DSANLPSystem:
    def __init__(self):
        print("ðŸš€ Initializing DSA NLP System...")

        # Initialize sentence transformer for embeddings
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… Sentence Transformer loaded")

        # Initialize ChromaDB for vector storage
        self.chroma_client = chromadb.Client()
        self.collection = self.chroma_client.create_collection(
            name="dsa_content",
            metadata={"hnsw:space": "cosine"}
        )
        print("âœ… ChromaDB initialized")

        # Initialize tokenizer and model for question generation
        # Using a smaller model that works well in Colab
        model_name = "microsoft/DialoGPT-small"  # Lightweight alternative
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        print("âœ… Language model loaded")

        # Initialize NLP tools
        self.nlp = spacy.load("en_core_web_sm")
        print("âœ… SpaCy model loaded")

        # User performance tracking
        self.user_stats = {
            'total_questions': 0,
            'correct_answers': 0,
            'difficulty_level': 'beginner',
            'weak_topics': [],
            'strong_topics': []
        }

        print("ðŸŽ‰ DSA NLP System ready!")

# Initialize the system
system = DSANLPSystem()

# ==========================================
# CELL 4: Document Processing Functions
# ==========================================

class DocumentProcessor:
    def __init__(self, nlp_model, embedder):
        self.nlp = nlp_model
        self.embedder = embedder

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF file"""
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            print(f"Error reading PDF: {e}")
            return ""

    def extract_text_from_docx(self, docx_path: str) -> str:
        """Extract text from Word document"""
        try:
            doc = Document(docx_path)
            text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
            return text
        except Exception as e:
            print(f"Error reading DOCX: {e}")
            return ""

    def chunk_text_by_topics(self, text: str, chunk_size: int = 512) -> List[Dict]:
        """Chunk text into semantic sections"""
        # Split by common DSA topic indicators
        topic_patterns = [
            r'\n\s*(Chapter|Section|Topic)\s*\d+[:\.]?\s*([^\n]+)',
            r'\n\s*([A-Z][A-Za-z\s]+(?:Algorithm|Tree|Graph|Sort|Search|Hash|Array|List|Stack|Queue))\s*\n',
            r'\n\s*(\d+\.\d*\s*[A-Z][^\n]+)\s*\n'
        ]

        chunks = []
        sentences = nltk.sent_tokenize(text)

        current_chunk = []
        current_topic = "General"
        current_length = 0

        for sentence in sentences:
            # Check for topic headers
            for pattern in topic_patterns:
                match = re.search(pattern, sentence, re.IGNORECASE)
                if match:
                    # Save current chunk if it exists
                    if current_chunk:
                        chunks.append({
                            'topic': current_topic,
                            'content': ' '.join(current_chunk),
                            'length': current_length,
                            'keywords': self.extract_keywords(' '.join(current_chunk))
                        })

                    # Start new chunk
                    current_topic = match.group(1) if len(match.groups()) == 1 else match.group(2)
                    current_chunk = []
                    current_length = 0
                    break

            current_chunk.append(sentence)
            current_length += len(sentence.split())

            # Split if chunk gets too large
            if current_length >= chunk_size:
                chunks.append({
                    'topic': current_topic,
                    'content': ' '.join(current_chunk),
                    'length': current_length,
                    'keywords': self.extract_keywords(' '.join(current_chunk))
                })
                current_chunk = []
                current_length = 0

        # Add final chunk
        if current_chunk:
            chunks.append({
                'topic': current_topic,
                'content': ' '.join(current_chunk),
                'length': current_length,
                'keywords': self.extract_keywords(' '.join(current_chunk))
            })

        return chunks

    def extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """Extract important keywords from text"""
        doc = self.nlp(text)

        # Extract important entities and noun phrases
        keywords = []

        # Named entities
        for ent in doc.ents:
            if ent.label_ in ['ORG', 'PRODUCT', 'EVENT', 'WORK_OF_ART']:
                keywords.append(ent.text.lower())

        # Important noun phrases and technical terms
        for token in doc:
            if (token.pos_ in ['NOUN', 'PROPN'] and
                len(token.text) > 2 and
                not token.is_stop and
                token.text.lower() not in keywords):
                keywords.append(token.text.lower())

        # Filter and return top keywords
        keyword_freq = {}
        for kw in keywords:
            keyword_freq[kw] = keyword_freq.get(kw, 0) + 1

        sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)
        return [kw[0] for kw in sorted_keywords[:max_keywords]]

    def classify_difficulty(self, content: str) -> str:
        """Classify content difficulty based on complexity indicators"""
        complexity_indicators = {
            'beginner': ['basic', 'introduction', 'simple', 'fundamental', 'overview'],
            'intermediate': ['algorithm', 'complexity', 'analysis', 'implementation', 'efficient'],
            'advanced': ['optimization', 'advanced', 'complex', 'sophisticated', 'theorem']
        }

        content_lower = content.lower()
        scores = {}

        for level, indicators in complexity_indicators.items():
            score = sum(1 for indicator in indicators if indicator in content_lower)
            scores[level] = score

        return max(scores.items(), key=lambda x: x[1])[0] if any(scores.values()) else 'beginner'

# Initialize document processor
doc_processor = DocumentProcessor(system.nlp, system.embedder)

# ==========================================
# CELL 5: Sample DSA Content (for testing)
# ==========================================

# Sample DSA content - In real implementation, you'd load from actual book
SAMPLE_DSA_CONTENT = """
Chapter 1: Introduction to Arrays

Arrays are fundamental data structures that store elements in contiguous memory locations.
The time complexity for accessing elements is O(1) since we can directly calculate the memory address.
However, insertion and deletion operations can take O(n) time in the worst case when elements need to be shifted.

Key operations on arrays include:
1. Access: O(1) time complexity
2. Search: O(n) for unsorted arrays, O(log n) for sorted arrays using binary search
3. Insertion: O(n) worst case, O(1) if inserting at the end
4. Deletion: O(n) worst case

Chapter 2: Binary Search Algorithm

Binary search is an efficient searching algorithm that works on sorted arrays.
It repeatedly divides the search space in half, achieving O(log n) time complexity.
The algorithm compares the target value with the middle element and eliminates half of the remaining elements.

Prerequisites for binary search:
1. Array must be sorted
2. Random access to elements (arrays work well)

Algorithm steps:
1. Set left = 0, right = n-1
2. While left <= right:
   - Calculate mid = (left + right) / 2
   - If arr[mid] == target, return mid
   - If arr[mid] < target, left = mid + 1
   - If arr[mid] > target, right = mid - 1
3. Return -1 if not found

Chapter 3: Linked Lists

Linked lists are linear data structures where elements are stored in nodes.
Each node contains data and a pointer to the next node in the sequence.
Unlike arrays, linked lists provide dynamic memory allocation and efficient insertion/deletion at any position.

Types of linked lists:
1. Singly linked list: Each node points to the next node
2. Doubly linked list: Each node has pointers to both next and previous nodes
3. Circular linked list: Last node points back to the first node

Time complexities:
- Access: O(n) - must traverse from head
- Search: O(n)
- Insertion: O(1) if position is known, O(n) to find position
- Deletion: O(1) if position is known, O(n) to find position

Chapter 4: Binary Trees

Binary trees are hierarchical data structures where each node has at most two children.
They are used in many applications including expression parsing, database indexing, and file systems.

Tree terminology:
- Root: The topmost node
- Leaf: A node with no children
- Height: Maximum distance from root to any leaf
- Depth: Distance from root to a particular node

Tree traversal methods:
1. Inorder (Left, Root, Right): Gives sorted output for BST
2. Preorder (Root, Left, Right): Used for creating copy of tree
3. Postorder (Left, Right, Root): Used for deleting tree

Binary Search Trees (BST):
- Left subtree contains nodes with values less than root
- Right subtree contains nodes with values greater than root
- Average time complexity: O(log n) for search, insert, delete
- Worst case: O(n) when tree becomes skewed

Chapter 5: Hash Tables

Hash tables use hash functions to map keys to array indices.
They provide average O(1) time complexity for search, insert, and delete operations.
Hash tables are widely used in database indexing, caching, and symbol tables.

Components of hash tables:
1. Hash function: Converts keys to array indices
2. Buckets: Array slots where values are stored
3. Collision resolution: Handling when multiple keys map to same index

Collision resolution techniques:
1. Chaining: Store multiple values in linked lists at same index
2. Open addressing: Find next available slot using probing
   - Linear probing: Check next slot sequentially
   - Quadratic probing: Check slots at quadratic intervals
   - Double hashing: Use second hash function

Load factor = Number of elements / Number of buckets
Optimal load factor is typically 0.75 for good performance.
"""

# Process the sample content
print("ðŸ“š Processing sample DSA content...")
chunks = doc_processor.chunk_text_by_topics(SAMPLE_DSA_CONTENT)

# Store in ChromaDB
for i, chunk in enumerate(chunks):
    # Generate embedding
    embedding = system.embedder.encode(chunk['content']).tolist()

    # Store in vector database
    system.collection.add(
        embeddings=[embedding],
        documents=[chunk['content']],
        metadatas=[{
            'topic': chunk['topic'],
            'difficulty': doc_processor.classify_difficulty(chunk['content']),
            'keywords': ','.join(chunk['keywords']),
            'chunk_id': i
        }],
        ids=[f"chunk_{i}"]
    )

print(f"âœ… Processed {len(chunks)} content chunks")
print("ðŸ“Š Sample topics found:", [chunk['topic'] for chunk in chunks])

# ==========================================
# CELL 6: Question Generation Engine
# ==========================================

class QuestionGenerator:
    def __init__(self, collection, embedder, tokenizer, model):
        self.collection = collection
        self.embedder = embedder
        self.tokenizer = tokenizer
        self.model = model

        # Question templates for different types
        self.templates = {
            'multiple_choice': [
                "What is the time complexity of {operation} in {data_structure}?",
                "Which of the following best describes {concept}?",
                "What is the main advantage of {structure1} over {structure2}?",
                "In {data_structure}, what happens when {scenario}?",
                "Which algorithm is most efficient for {problem}?"
            ],
            'conceptual': [
                "What is the primary characteristic of {concept}?",
                "How does {algorithm} work?",
                "What are the prerequisites for {algorithm}?",
                "What is the difference between {concept1} and {concept2}?"
            ],
            'implementation': [
                "What is the first step in implementing {algorithm}?",
                "Which data structure is best for {use_case}?",
                "What condition terminates {algorithm}?"
            ]
        }

        # Common distractors for different topics
        self.distractors = {
            'time_complexity': ['O(1)', 'O(log n)', 'O(n)', 'O(n log n)', 'O(nÂ²)', 'O(2^n)'],
            'data_structures': ['Array', 'Linked List', 'Stack', 'Queue', 'Tree', 'Graph', 'Hash Table'],
            'algorithms': ['Binary Search', 'Linear Search', 'Bubble Sort', 'Quick Sort', 'Merge Sort', 'DFS', 'BFS']
        }

    def retrieve_relevant_content(self, topic: str, difficulty: str = None, limit: int = 3) -> List[Dict]:
        """Retrieve relevant content chunks for question generation"""
        # Create query embedding
        query_embedding = self.embedder.encode(topic).tolist()

        # Search similar content
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=limit,
            where={"difficulty": difficulty} if difficulty else None
        )

        retrieved_content = []
        for i in range(len(results['documents'][0])):
            retrieved_content.append({
                'content': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i] if 'distances' in results else 0
            })

        return retrieved_content

    def extract_key_concepts(self, content: str) -> Dict:
        """Extract key concepts from content for question generation"""
        concepts = {
            'time_complexities': re.findall(r'O\([^)]+\)', content),
            'data_structures': re.findall(r'\b(?:array|linked list|stack|queue|tree|graph|hash table)s?\b', content, re.IGNORECASE),
            'algorithms': re.findall(r'\b(?:binary search|linear search|sort|traversal|insertion|deletion)(?:\s+\w+)*\b', content, re.IGNORECASE),
            'operations': re.findall(r'\b(?:search|insert|delete|access|traversal)(?:ing|ion|ed)?\b', content, re.IGNORECASE)
        }

        return concepts

    def generate_mcq_options(self, correct_answer: str, option_type: str, num_options: int = 4) -> List[str]:
        """Generate multiple choice options including the correct answer"""
        if option_type in self.distractors:
            available_distractors = [d for d in self.distractors[option_type] if d.lower() != correct_answer.lower()]
            distractors = random.sample(available_distractors, min(num_options - 1, len(available_distractors)))
        else:
            distractors = [f"Option {i+1}" for i in range(num_options - 1)]

        options = [correct_answer] + distractors
        random.shuffle(options)

        correct_index = options.index(correct_answer)
        return options, correct_index

    def generate_question(self, topic: str, difficulty: str = 'beginner') -> Dict:
        """Generate a complete MCQ question"""
        # Retrieve relevant content
        relevant_content = self.retrieve_relevant_content(topic, difficulty)

        if not relevant_content:
            return self.generate_fallback_question(topic, difficulty)

        # Select best content chunk
        best_content = relevant_content[0]
        content_text = best_content['content']

        # Extract concepts from content
        concepts = self.extract_key_concepts(content_text)

        # Generate question based on available concepts
        if concepts['time_complexities']:
            return self.generate_complexity_question(content_text, concepts, difficulty)
        elif concepts['data_structures']:
            return self.generate_concept_question(content_text, concepts, difficulty)
        else:
            return self.generate_general_question(content_text, concepts, difficulty)

    def generate_complexity_question(self, content: str, concepts: Dict, difficulty: str) -> Dict:
        """Generate time complexity focused question"""
        complexities = concepts['time_complexities']
        operations = concepts['operations']

        if complexities and operations:
            complexity = random.choice(complexities)
            operation = random.choice(operations)

            question = f"What is the time complexity of {operation} mentioned in the content?"
            options, correct_index = self.generate_mcq_options(complexity, 'time_complexity')

            return {
                'question': question,
                'options': options,
                'correct_answer': correct_index,
                'explanation': f"According to the content, {operation} has {complexity} time complexity.",
                'topic': concepts.get('data_structures', ['Unknown'])[0] if concepts.get('data_structures') else 'General',
                'difficulty': difficulty,
                'type': 'time_complexity'
            }

        return self.generate_general_question(content, concepts, difficulty)

    def generate_concept_question(self, content: str, concepts: Dict, difficulty: str) -> Dict:
        """Generate conceptual question"""
        data_structures = concepts['data_structures']

        if data_structures:
            ds = random.choice(data_structures).title()

            # Extract key information about the data structure
            sentences = content.split('.')
            relevant_sentences = [s for s in sentences if ds.lower() in s.lower()]

            if relevant_sentences:
                key_info = relevant_sentences[0].strip()

                question = f"What is a key characteristic of {ds} mentioned in the content?"

                # Create options based on content
                options = [
                    key_info.split(',')[0] if ',' in key_info else key_info,
                    "Requires sorted data",
                    "Uses recursive approach only",
                    "Has constant space complexity"
                ]
                random.shuffle(options)
                correct_index = 0

                return {
                    'question': question,
                    'options': options,
                    'correct_answer': correct_index,
                    'explanation': f"The content states: {key_info}",
                    'topic': ds,
                    'difficulty': difficulty,
                    'type': 'conceptual'
                }

        return self.generate_general_question(content, concepts, difficulty)

    def generate_general_question(self, content: str, concepts: Dict, difficulty: str) -> Dict:
        """Generate general question from content"""
        sentences = content.split('.')
        key_sentence = random.choice([s for s in sentences if len(s.strip()) > 20])

        # Extract the main subject
        words = key_sentence.split()
        subject = ' '.join(words[:3]) if len(words) >= 3 else key_sentence[:30]

        question = f"According to the content, what can be said about {subject.lower()}?"

        options = [
            key_sentence.strip(),
            "It has exponential time complexity",
            "It requires additional memory allocation",
            "It works only with numeric data"
        ]

        random.shuffle(options)
        correct_index = options.index(key_sentence.strip())

        return {
            'question': question,
            'options': options,
            'correct_answer': correct_index,
            'explanation': f"The content explicitly states: {key_sentence.strip()}",
            'topic': 'General',
            'difficulty': difficulty,
            'type': 'general'
        }

    def generate_fallback_question(self, topic: str, difficulty: str) -> Dict:
        """Generate fallback question when no relevant content found"""
        fallback_questions = {
            'arrays': {
                'question': "What is the time complexity of accessing an element in an array by index?",
                'options': ["O(1)", "O(log n)", "O(n)", "O(nÂ²)"],
                'correct_answer': 0,
                'explanation': "Array elements can be accessed directly using their index, making it O(1) time complexity."
            },
            'search': {
                'question': "Which search algorithm requires the array to be sorted?",
                'options': ["Binary Search", "Linear Search", "Jump Search", "Hash Search"],
                'correct_answer': 0,
                'explanation': "Binary search requires the array to be sorted to work correctly."
            }
        }

        # Select appropriate fallback
        for key, q in fallback_questions.items():
            if key.lower() in topic.lower():
                return {**q, 'topic': topic, 'difficulty': difficulty, 'type': 'fallback'}

        # Default fallback
        return {
            'question': f"What is an important concept related to {topic}?",
            'options': ["Data organization", "Memory allocation", "Algorithm efficiency", "All of the above"],
            'correct_answer': 3,
            'explanation': f"All mentioned concepts are important when studying {topic}.",
            'topic': topic,
            'difficulty': difficulty,
            'type': 'fallback'
        }

# Initialize question generator
question_gen = QuestionGenerator(
    system.collection,
    system.embedder,
    system.tokenizer,
    system.model
)

print("âœ… Question Generator initialized")

# ==========================================
# CELL 7: Adaptive Learning System
# ==========================================

class AdaptiveLearning:
    def __init__(self):
        self.user_performance = {
            'questions_answered': 0,
            'correct_answers': 0,
            'topic_performance': {},
            'difficulty_performance': {'beginner': [], 'intermediate': [], 'advanced': []},
            'recent_performance': [],
            'current_difficulty': 'beginner'
        }
        self.performance_window = 10  # Consider last N questions for adaptation

    def update_performance(self, question_data: Dict, user_answer: int, is_correct: bool):
        """Update user performance metrics"""
        self.user_performance['questions_answered'] += 1
        if is_correct:
            self.user_performance['correct_answers'] += 1

        # Update topic performance
        topic = question_data.get('topic', 'General')
        if topic not in self.user_performance['topic_performance']:
            self.user_performance['topic_performance'][topic] = {'correct': 0, 'total': 0}

        self.user_performance['topic_performance'][topic]['total'] += 1
        if is_correct:
            self.user_performance['topic_performance'][topic]['correct'] += 1

        # Update difficulty performance
        difficulty = question_data.get('difficulty', 'beginner')
        self.user_performance['difficulty_performance'][difficulty].append(1 if is_correct else 0)

        # Update recent performance
        self.user_performance['recent_performance'].append(1 if is_correct else 0)
        if len(self.user_performance['recent_performance']) > self.performance_window:
            self.user_performance['recent_performance'].pop(0)

    def get_current_accuracy(self) -> float:
        """Get current overall accuracy"""
        if self.user_performance['questions_answered'] == 0:
            return 0.0
        return self.user_performance['correct_answers'] / self.user_performance['questions_answered']

    def get_recent_accuracy(self) -> float:
        """Get accuracy for recent questions"""
        recent = self.user_performance['recent_performance']
        if not recent:
            return 0.0
        return sum(recent) / len(recent)

    def get_topic_accuracy(self, topic: str) -> float:
        """Get accuracy for specific topic"""
        if topic not in self.user_performance['topic_performance']:
            return 0.0

        topic_stats = self.user_performance['topic_performance'][topic]
        if topic_stats['total'] == 0:
            return 0.0

        return topic_stats['correct'] / topic_stats['total']

    def adapt_difficulty(self) -> str:
        """Adapt difficulty based on performance"""
        recent_accuracy = self.get_recent_accuracy()
        current_difficulty = self.user_performance['current_difficulty']

        # Adaptation rules
        if recent_accuracy >= 0.8 and len(self.user_performance['recent_performance']) >= 5:
            if current_difficulty == 'beginner':
                self.user_performance['current_difficulty'] = 'intermediate'
            elif current_difficulty == 'intermediate':
                self.user_performance['current_difficulty'] = 'advanced'
        elif recent_accuracy <= 0.4 and len(self.user_performance['recent_performance']) >= 5:
            if current_difficulty == 'advanced':
                self.user_performance['current_difficulty'] = 'intermediate'
            elif current_difficulty == 'intermediate':
                self.user_performance['current_difficulty'] = 'beginner'

        return self.user_performance['current_difficulty']

    def get_weak_topics(self, threshold: float = 0.5) -> List[str]:
        """Identify topics where user performance is below threshold"""
        weak_topics = []
        for topic, stats in self.user_performance['topic_performance'].items():
            if stats['total'] >= 2:  # At least 2 questions answered
                accuracy = stats['correct'] / stats['total']
                if accuracy < threshold:
                    weak_topics.append(topic)

        return weak_topics

    def recommend_next_topic(self) -> str:
        """Recommend next topic based on performance"""
        weak_topics = self.get_weak_topics()

        if weak_topics:
            # Focus on weakest topic
            topic_accuracies = {topic: self.get_topic_accuracy(topic) for topic in weak_topics}
            return min(topic_accuracies.items(), key=lambda x: x[1])[0]

        # If no weak topics, suggest topics with fewer questions
        topic_counts = {topic: stats['total']
                       for topic, stats in self.user_performance['topic_performance'].items()}

        if topic_counts:
            return min(topic_counts.items(), key=lambda x: x[1])[0]

        # Default topics to explore
        default_topics = ['Arrays', 'Binary Search', 'Linked Lists', 'Trees', 'Hash Tables']
        return random.choice(default_topics)

    def get_performance_summary(self) -> Dict:
        """Get comprehensive performance summary"""
        return {
            'overall_accuracy': self.get_current_accuracy(),
            'recent_accuracy': self.get_recent_accuracy(),
            'questions_answered': self.user_performance['questions_answered'],
            'current_difficulty': self.user_performance['current_difficulty'],
            'topic_performance': {
                topic: {
                    'accuracy': stats['correct'] / stats['total'] if stats['total'] > 0 else 0,
                    'questions': stats['total']
                }
                for topic, stats in self.user_performance['topic_performance'].items()
            },
            'weak_topics': self.get_weak_topics(),
            'recommended_topic': self.recommend_next_topic()
        }

# Initialize adaptive learning system
adaptive_system = AdaptiveLearning()

print("âœ… Adaptive Learning System initialized")

# ==========================================
# CELL 8: Main Learning Interface
# ==========================================

class DSALearningInterface:
    def __init__(self, question_generator, adaptive_system):
        self.question_gen = question_generator
        self.adaptive = adaptive_system
        self.current_question = None
        self.session_stats = {'questions': 0, 'correct': 0}

    def generate_next_question(self, topic: str = None) -> Dict:
        """Generate next question based on adaptive system"""
        if not topic:
            topic = self.adaptive.recommend_next_topic()

        difficulty = self.adaptive.adapt_difficulty()

        self.current_question = self.question_gen.generate_question(topic, difficulty)
        return self.current_question

    def submit_answer(self, answer_index: int) -> Dict:
        """Process user answer and provide feedback"""
        if not self.current_question:
            return {"error": "No active question"}

        is_correct = answer_index == self.current_question['correct_answer']

        # Update adaptive system
        self.adaptive.update_performance(self.current_question, answer_index, is_correct)

        # Update session stats
        self.session_stats['questions'] += 1
        if is_correct:
            self.session_stats['correct'] += 1

        # Prepare feedback
        feedback = {
            'is_correct': is_correct,
            'correct_answer': self.current_question['options'][self.current_question['correct_answer']],
            'user_answer': self.current_question['options'][answer_index],
            'explanation': self.current_question['explanation'],
            'session_accuracy': self.session_stats['correct'] / self.session_stats['questions'],
            'overall_performance': self.adaptive.get_performance_summary()
        }

        return feedback

    def get_performance_dashboard(self) -> Dict:
        """Get comprehensive performance dashboard"""
        return {
            'session_stats': self.session_stats,
            'adaptive_stats': self.adaptive.get_performance_summary(),
            'current_difficulty': self.adaptive.user_performance['current_difficulty'],
            'recommendations': {
                'next_topic': self.adaptive.recommend_next_topic(),
                'weak_topics': self.adaptive.get_weak_topics(),
                'suggested_difficulty': self.adaptive.adapt_difficulty()
            }
        }

# Initialize learning interface
learning_interface = DSALearningInterface(question_gen, adaptive_system)

print("âœ… Learning Interface initialized")