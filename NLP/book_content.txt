Chapter 1: Introduction to Arrays

Arrays are fundamental data structures that store elements in contiguous memory locations.
The time complexity for accessing elements is O(1) since we can directly calculate the memory address.
However, insertion and deletion operations can take O(n) time in the worst case when elements need to be shifted.

Key operations on arrays include:
1. Access: O(1) time complexity
2. Search: O(n) for unsorted arrays, O(log n) for sorted arrays using binary search
3. Insertion: O(n) worst case, O(1) if inserting at the end
4. Deletion: O(n) worst case

Chapter 2: Binary Search Algorithm

Binary search is an efficient searching algorithm that works on sorted arrays.
It repeatedly divides the search space in half, achieving O(log n) time complexity.
The algorithm compares the target value with the middle element and eliminates half of the remaining elements.

Prerequisites for binary search:
1. Array must be sorted
2. Random access to elements (arrays work well)

Algorithm steps:
1. Set left = 0, right = n-1
2. While left <= right:
   - Calculate mid = (left + right) / 2
   - If arr[mid] == target, return mid
   - If arr[mid] < target, left = mid + 1
   - If arr[mid] > target, right = mid - 1
3. Return -1 if not found

Chapter 3: Linked Lists

Linked lists are linear data structures where elements are stored in nodes.
Each node contains data and a pointer to the next node in the sequence.
Unlike arrays, linked lists provide dynamic memory allocation and efficient insertion/deletion at any position.

Types of linked lists:
1. Singly linked list: Each node points to the next node
2. Doubly linked list: Each node has pointers to both next and previous nodes
3. Circular linked list: Last node points back to the first node

Time complexities:
- Access: O(n) - must traverse from head
- Search: O(n)
- Insertion: O(1) if position is known, O(n) to find position
- Deletion: O(1) if position is known, O(n) to find position

Chapter 4: Binary Trees

Binary trees are hierarchical data structures where each node has at most two children.
They are used in many applications including expression parsing, database indexing, and file systems.

Tree terminology:
- Root: The topmost node
- Leaf: A node with no children
- Height: Maximum distance from root to any leaf
- Depth: Distance from root to a particular node

Tree traversal methods:
1. Inorder (Left, Root, Right): Gives sorted output for BST
2. Preorder (Root, Left, Right): Used for creating copy of tree
3. Postorder (Left, Right, Root): Used for deleting tree

Binary Search Trees (BST):
- Left subtree contains nodes with values less than root
- Right subtree contains nodes with values greater than root
- Average time complexity: O(log n) for search, insert, delete
- Worst case: O(n) when tree becomes skewed

Chapter 5: Hash Tables

Hash tables use hash functions to map keys to array indices.
They provide average O(1) time complexity for search, insert, and delete operations.
Hash tables are widely used in database indexing, caching, and symbol tables.

Components of hash tables:
1. Hash function: Converts keys to array indices
2. Buckets: Array slots where values are stored
3. Collision resolution: Handling when multiple keys map to same index

Collision resolution techniques:
1. Chaining: Store multiple values in linked lists at same index
2. Open addressing: Find next available slot using probing
   - Linear probing: Check next slot sequentially
   - Quadratic probing: Check slots at quadratic intervals
   - Double hashing: Use second hash function

Load factor = Number of elements / Number of buckets
Optimal load factor is typically 0.75 for good performance.

Chapter 6: Dynamic Programming

Dynamic Programming is an algorithmic paradigm that solves complex problems by breaking them down into simpler subproblems.
It is applicable to problems exhibiting the properties of overlapping subproblems and optimal substructure.

Key characteristics:
1. Overlapping Subproblems: The problem can be broken down into subproblems which are reused several times
2. Optimal Substructure: An optimal solution can be constructed from optimal solutions of its subproblems

Approaches:
1. Top-down (Memoization): Solve the problem recursively and store results of subproblems
2. Bottom-up (Tabulation): Solve all related subproblems first, then solve the larger problem

Common DP problems:
- Fibonacci sequence
- Longest Common Subsequence
- 0/1 Knapsack Problem
- Coin Change Problem
- Edit Distance

Time complexity: Usually O(n²) or O(n³) depending on the number of states
Space complexity: O(n) or O(n²) for storing the DP table

Chapter 7: Graph Algorithms

Graphs are non-linear data structures consisting of vertices (nodes) and edges.
They are used to represent networks, relationships, and many real-world problems.

Graph representations:
1. Adjacency Matrix: 2D array where matrix[i][j] represents edge between vertex i and j
2. Adjacency List: Array of lists where each list contains neighbors of a vertex

Graph traversal algorithms:
1. Depth-First Search (DFS): Explores as far as possible along each branch before backtracking
2. Breadth-First Search (BFS): Explores all neighbors at the current depth before moving to next depth

Shortest path algorithms:
1. Dijkstra's Algorithm: Finds shortest path from source to all vertices (non-negative weights)
2. Bellman-Ford Algorithm: Handles negative weights, detects negative cycles
3. Floyd-Warshall Algorithm: Finds shortest paths between all pairs of vertices

Minimum Spanning Tree:
1. Kruskal's Algorithm: Greedy approach using Union-Find
2. Prim's Algorithm: Greedy approach starting from arbitrary vertex

Time complexities vary based on representation and algorithm used.